
### adam那么好，为何还对sgd 念念不忘


### sgd，adam， adamgrad

只是相当于在SGD基础上增加了各类学习率的主动控制。


发展历史
SGD - SGDM - AdaGrad - Adam - Nadam

定义
待优化参数 w
目标函数 f(w)
初始学习率 alpha

w_(t+1) = w_t - alpha * g_t

epoch t:
1）计算目标函数关于当前参数的梯度  g_t = Nabla f(w_t)

2）根据历史梯度计算一阶动量和二阶动量   m_t = phi(g_1, g_2,,, g_t), V_t = psi(g_1, g_2,,,g_t)

3）计算当前时刻的下降梯度  eta_t = alpha * m_t / sqrt(V_t)

4）根据下降梯度进行梯度更新  w_(t+1) = w_t - eta_t


对于各个优化算法，3/4部分是一样的，只是1/2部分不同。


SGD
SGD没有动量的概念，即 m_t = g_t, V_t = I^2
eta_t = alpha * g_t

SGD局限性：
下降速度慢
可能会在沟壑的两边持续震荡，停留在一个局部最优点


SGD with Momentum
为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，则利用惯性跑得快一些。加入一阶动量。

m_t = beta_1 * m_(t-1) + (1- beta_1) * g_t

一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近 1/（1-beta_1) 个时刻的梯度向量和的平均值。

t时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前积累的下降方向决定(占主导 beta_1=0.9 经验值)



SGD with Nestrerov Acceleration
SGD还有一个问题是困在局部最优的沟壑里面震荡。假如你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。

我们知道在时刻t的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。

g_t = Nabla f(w_t - alpha * m_(t-1)/sqrt(V_(t-1)) )



AdaGrad
二阶动量的出现，才意味着自适应学习率  优化算法的到来。
SGD及其系列，以同样的学习率更新每个参数，但深度神经网络往往包含着大量的参数。
这些参数并不总是能用上（如大规模 embedding）。
对于经常更新的参数，我们已经积累了大量关于它的只是，不希望被单个样本影响太大，希望学习率慢一些；
对于偶尔更新的参数，我们积累信息太少，希望能从这个样本上多学一些，即学习速率大一些。

二阶动量，该维度上，迄今为止所有梯度值的平方和。
V_t = \sum_(r=1)^(t) g_r^2

eta_t = alpha * m_t / sqrt(V_t)

相当于学习率从 alpha 变成 alpha/sqrt(V_t)
一般为了避免分母为0，会在分母上加一个小的平滑项，因此 sqrt(V_t) 是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小。

这一方法在稀疏数据场景下表现非常好。也存在一些问题，因为 sqrt(V_t) 是单调递增的，会使得学习率 单调递减至0，可能会使得训练过程提前结束。即便后续还有数据也无法学习到必要的知识。。






AdaDelta/ RMSProp
由于AdaGrad 单调递减的学习率过于激进，对其改进：不累计全部历史梯度，知识关注过去一段时间窗口的下降梯度/

V_t = beta_2 * V_(t-1) + (1-beta_2)* g_t^2
避免了二阶动量持续累积，导致训练过程提前结束的问题了。


Adam
Adam和 Nadam是前述方法的集大成者。
AGD-M在SGD基础上增加 一阶动量
AdaGram 和AdaDelta 在 SGD基础上增加二阶动量。

把一阶动量和二阶动量都用起来，就是 Adam了。 adaptive + momentum。

m_t = beta_1 * m_(t-1) + (1- beta_1) * g_t

V_t = beta_2 * V_(t-1) + (1-beta_2)* g_t^2

beta_1 控制一阶动量，beta_2 控制二阶动量。经验值 beta_1 = 0.99, beta_2 = 0.999

初始化如果m_0 = 0, V_0 = 0,会导致 m_t, V_t 都会接近于0，这个估计是有问题的。
因此需要修正
hat( m_t) = m_t/(1- beta_1^t)
hat( V_t) = V_t/(1- beta_2^t)



Nadam
Nadam = adam + Nesterov

g_t = Nabla f(w_t - alpha * m_(t-1)/sqrt(V_t) )



一阶动量和二阶动量都是按照指数移动平均值进行计算的。




### adam的局限性
局限性
1）adam 在某些情况下可能不收敛
2）adam 可能错过全局最优解


1）adam 在某些情况下可能不收敛
SGD没有用到二阶动量，因此学习率是恒定的（实际使用过程中会采用学习率衰减策略，因此学习率递减）

AdaGrad的二阶动量不断累积，单调递增，因此学习率是单调递减的。因此，这两类算法会使得学习率不断递减，最终收敛到0，模型也得以收敛。

AdaDelta和Adam则不然。二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得 V_t 可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。

这篇文章也给出了一个修正的方法。由于Adam中的学习率主要是由二阶动量控制的，为了保证算法的收敛，可以对二阶动量的变化进行控制，避免上下波动。




2）adam 可能错过全局最优解
深度神经网络往往包含大量的参数，在这样一个维度极高的空间内，非凸的目标函数往往起起伏伏，拥有无数个高地和洼地。有的是高峰，通过引入动量可能很容易越过；但有些是高原，可能探索很多次都出不来，于是停止了训练。

前文提到的吐槽Adam最狠的 The Marginal Value of Adaptive Gradient Methods in Machine Learning 。文中说到，同样的一个优化问题，不同的优化算法可能会找到不同的答案，但自适应学习率的算法往往找到非常差的答案。他们通过一个特定的数据例子说明，自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。

Improving Generalization Performance by Switching from Adam to SGD，进行了实验验证。他们CIFAR-10数据集上进行测试，Adam的收敛速度比SGD要快，但最终收敛的结果并没有SGD好。他们进一步实验发现，主要是后期Adam的学习率太低，影响了有效的收敛。他们试着对Adam的学习率的下界进行控制，发现效果好了很多。

于是他们提出了一个用来改进Adam的方法：前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。这一方法以前也被研究者们用到，不过主要是根据经验来选择切换的时机和切换后的学习率。这篇文章把这一切换过程傻瓜化，给出了切换SGD的时机选择方法，以及学习率的计算方法，效果看起来也不错。



### adam好还是sgd好
去看学术会议中的各种paper，用SGD的很多，Adam的也不少。

算法固然美好，数据才是根本。




### adam + sgd组合粗略，及tricks
eta_t = (alpha/ sqrt(V_t)) * m_t
前半部分是实际的学习率（也即下降步长），后半部分是实际的下降方向。

SGD算法的下降方向就是该位置的梯度方向的反方向，带一阶动量的SGD的下降方向则是该位置的一阶动量方向。自适应学习率类优化算法为每个参数设定了不同的学习率，在不同维度上设定不同步长，因此其下降方向是缩放过（scaled）的一阶动量方向。

由于下降方向的不同，可能导致不同算法到达完全不同的局部最优点。

不同优化算法的优劣依然是未有定论的争议话题。


### FTRL稀疏性
在线学习和CTR常常会用到逻辑回归（ Logistic Regression），而传统的批量（batch）算法无法有效地处理超大规模的数据集和在线数据流，
google先后三年时间（2010年-2013年）从理论研究到实际工程化实现的FTRL（Follow-the-regularized-Leader）算法，
在处理诸如逻辑回归之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色。



批处理bacth的离线机器学习方法在每次迭代计算的过程中，需要把全部的训练数据加载到内存中计算（例如计算全局梯度）， 虽然有分布式大规模的机器学习平台，在某种程度上批处理方法对训练样本的数量还是有限制的，online learning不需要cache所有数据，以流式的处理方式可以处理任意数量的样本。研究online learning有两个角度，在线凸优化和在线Bayesian。

在线凸优化方法有很多，像FOBOS算法、RDA、FTRL等；在线Bayesian 方面有比如AdPredictor 算法、基于内容的在线矩阵分解算法等，有兴趣的可以多了解下。包括我们在实际项目中会将FTRL做相应的改进优化，KDD竞赛也是用的FTRL不少。







