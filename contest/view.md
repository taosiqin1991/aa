

### 平台或者软件涉及需要考虑到到
1.设计要分层，底层通用函数，关键模块，上层应用层
2.可扩展性好，增加一个模块方便
3.日志系统要全面，能监测底层、算法层、应用层各个地方的信息，便于追踪bug。要设计极端情况（如最底层caffe的bug导致软件崩溃）下要留下尽可能多的日志信息
4.调试信息要能方便的打开或者关闭。
5.二次开发接口的设计要尽量支持各种语言，需要思考设计
6.接口设计需要考虑完备性、必要性。


### 经验性能
wide&deep的性能比 deepfm好（京东面试官提到，在知乎也看到某位工程师实验观测到此现象）



### opencv
cv::Mat的 resize，scale，blur，rotate，shift，crop，flip操作。
最大值最小值统计
通道变换函数

找轮廓
最小包围核
hough霍夫直线检测
sift 特征点匹配


### caffe框架
数据Blob，Layer，Net，Solver
静态图模型，参数都是用proto生成，然后转化为Blob，来存储网络参数和feature_map.
Blob有NCHW维度，有data和diff，都有cpu和gpu的接口函数。底层用状态机自动处理cpu/gpu数据同步。
网络模型参数，序列化保存，可以选择文本形式或者二进制或者其他。

Net有很多Layer组成，Net可以有自己的Blob。
Layer有自己的Blob
Solver中来处理Net的train过程，在net_forward,net_backward后处理梯度更新的操作。

caffe中不同的层和不同的solver都通过工厂模式创建出来。
Net和Solver都提供了回调函数，来自主定制net_forward前，net_backward后等的操作。


### CNN结构的设计
1）参考某种backbone
2）考虑net、loss、solver的选择
3）考虑net中第一层的卷积核大小、带孔卷积、deformable conv、conv depthwise。
4）考虑net整体的感受野，感受野不能太小。
5）如果多个loss，多个loss之间的 loss_weight超参可以调整。
6）上采样中deconv还是简单的双线性插值选择。
其他


### CNN整体流程
train： 数据，选择合适的resize，数据增强(flip_up, flip_left, rotate_90, rotate, shift, blur, brightness7种)，采样框的选择或者crop的选择，训练。保存模型。
test：加载模型，对于数据，同样的resize，net_forward, 后处理包括阈值、面积过滤等。


### CNN采样，推荐中采样
Faster-RCNN中采样设计
1）anchor框与目标框iou大于0.7 的作为正样本训练（加上每个目标框对应的最大的anchor框），iou小于0.3的作为负样本进行训练。
由于大量的负样本，正样本数量很少。所以在训练时要提升在一个batch里面正样本数量和比例。
可以按照能有多少正样本全选。然后1：2数量比选择负样本数量。

Faster-RCNN模型是两阶段模型，有两个分类过程。第一个分类是区分框是目标还是背景。第二个分类是细分类，输出具体的类别。分类过程就必然要需要每类数量是均衡的。

Faster-RCNN的训练的一个简单策略是：第一阶段选择全部正样本，按1：2随机选择负样本，来训练第一个分类器。第二阶段按第一阶段的结果有多少正样本和负样本直接输入，不做数量调整和重复。两者一起训练的过程中，前期第一个分类器效果差，第二个分类器效果随缘；慢慢第一个分类器效果好了，输出了更多的目标框可以给第二个分类器训练，第二个分类器的细分类性能也逐步提升。

缺陷检测分割模块的采样设计
缺陷的面积在整图中占比很小，如5%，可以采用合适的crop策略。1）增加缺陷在图像中的占比 2）省显存。

推荐系统中采样设计





### 推荐embedding
推荐广告搜索中，embedding是非常重要的。
一方面发现模型结构和参数量跟CNN比简直是小儿科so easy，一方面是推荐系统中有大量可用的特征，随便加加特征加一个浅层网络，可能性能就已经很好了。再往深加深层网络性能没啥提升，所以在推荐系统里，结构都so easy。

1） embedding层，高维度稀疏向量向低维稠密特征向量的转换
2）预训练的embedding特征向量，与其他特征向量连接后一同输入网络进行训练
3）计算用户和物品的embedding相似度，embedding可以直接作为推荐系统或者计算广告系统的召回层或者召回方法之一。


高维度稀疏特征向量天然不适合多层dnn。
推荐中：输入层，embedding层，全连接层。
embedding层的本质是学习一个M*N的权重参数矩阵，M是词典数量，N是输出的低维度。通常，M是巨大的如1e5，N=128.此层参数量巨大，如果任务中数据量本身不够大，可以采用预训练方式。

用户和物品的embedding通常是比较稳定的，可以以周为频率来进行更新。 
排序往往以天来进行训练。


deep&wide中deep部分的输入是one-hot表示的sparse features。每个embedding维度为32. 拼接embedding维度为1200(一组解 1200 = 32 * 30 + 240)
wide部分特征有两个，是曝光app和用户已安装app。





### dbscan与k-means聚类区别
k-means是基于划分的聚类，需要指定K。交替生成类中心和各样本属于哪一类。k-means的初始值由较大影响，不好。这点dbscan更好。

dbscan是基于密度的聚类。设定领域d的值。由密度可达关系导出的最大密度相连的样本集合。


### 熵和交叉熵cross-entropy
1）熵的定义来自信息论，香农提出，熵指的是无损编码事件中信息的最小平均编码长度。
平均编码长度越小越高效。
是无损编码，信息不能丢失。
2）熵的公式  entropy = -\sum plogp
假设一个信息事件有8种可能状态，且各状态等可能性，即概率是1/8，需要多少位来编码呢，是 -logp=3。然后再计算平均。
3) 熵的直观解释( 物理意义)

如果熵比较大(), 意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低。因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。

当一个罕见的信息到达时，比一个常见的信息有更多的信息量，因为它排除了别的很多的可能性。
在天气的例子中，rainy发生的概率时12.5%，当接收到该信息时，我们减少了87.5%的并不确定性(Fine, Clouy, Snow), 在概率计算/联合概率比较有用。   

一个离散变量i的概率分布是p(i)
entropy = - \sum_i plogp
entropy = -\int plogp dx

交叉熵是用来预测真实的概率分布问题的。
假定Q是真实概率分布，P是期望概率分布。用期望的概率分布去接近真实概率分布Q。
H(p, q) = -qlogp, 对于期望用真实的概率，对于编码长度用预测的概率分布。

因为熵是平均最小编码长度，所以交叉熵 >= 熵
当损失函数定义为交叉熵时，loss减小，则预测概率分布p 越来越接近 真实概率分布q，当完全正确，交叉熵的值为0. 因此可以使用交叉熵作为损失函数。



### conv层，pooling层的数学和物理意义，以及其作用
1）计算过程
卷积：局部连接，能提取局部的二维特征，权值共享。
池化层：有一定的拉伸不变性/旋转不变性。

卷积 + 池化：有一定的平移不变性 和 拉伸不变性。

2）关于平移、旋转、拉伸不变性
3）


### batchnorm原理，意义，以及caffe中实现逻辑
1）原理： BatchNorm尝试去解决 ICS问题，Internal Coveriate Shift内部变量分布迁移问题。
BN改变了网络的中间层的输入分布

2）数学公式上，BN 在一个batch内计算每一层的均值、标准差，然后减去均值，除以标准差，将分布转换为均值为0，标准差为1的标准分布。
然后再一个scale操作(gamma * x + beta)。这是BN中最重要的部分，将分布变成了均值为beta，标准差为gamma的分布。
整体上看，BN层的作用是通过参数控制了每一层输出的均值和标准差。

3）BN并没有解决掉ICS问题，而是引入参数gamma和beta 来调节中间层输出的均值和标准差。gamma和beta会在训练过程中不断更新，意味着均值和标准差也在不断变化。

BN为何有用呢，GAN文章lan Goodfellow给出一个可能的解释：
a) 神经网络在训练过程中，更新了某一层得到权重参数，后续每一层网络的输出都可能发生变化，最终引起loss值的变化。
当没有BN层时，loss的收敛需要精心设计 权重初始化方法 和超参调节方法以及漫长的训练时间。
当在各层之间加入BN层时，某层的输出仅由两个参数gamma 和beta 决定，使用梯度下降法优化参数 时，优化方法只需要调节两个参数的值来控制各层的输出，而不需要调节各层的全部参数。
这样提升了收敛速度，避免了小心翼翼的参数初始化和超参调节过程。


BN使用需要注意的细节
1）将BN放在激活层之后可能有更好的效果。尽管BN原论文将BN放在了relu之前。
直观理解：BN输出再经过激活层才能到达下一层，这样BN就无法完全控制下一层的输入分布，将BN层放在激活层之后就不会有这样的问题。
2）假如推理过程没有batch怎么办？ 推理过程batch=1
主流深度学习的解决方法是：在训练时跟踪记录每一个batch 的均值和标准差，并使用这些值对全部样本的均值和标准差作无偏估计，公式如下。
E(x) = E_b[\mu_b]
Var(x) = m/(m-1) * E_b[\sigma_b^2], 这个m = NHW，是一个batch中的值。

推断再利用
y = \gamma * (x - E(x)) /(sqrt(Var(x) + \eta) + \beta

caffe中实现mean, var的无偏估计计算，是通过 moving average滑动平均。
caffe中的BN是利用BN和scale两个层来实现的。caffe的BN层有三个blob，分别是mean，var，滑动平均因子moving average factor。
caffe并不是简简单单将每次计算的mean和var累加，而是把前一次计算的mean和var的影响减小(乘以一个小于1的变量)，再加上本次计算的结果。
E_t = \beta * E_(t-1) + (1-\beta) Z_t
V_t = \beta * V_(t-1) + (1-\beta) * m/(m-1) * Y_t, Y_t是当前值。当m=1中间的系数用1即可。

滑动系数也在更新，假定moving_average_fraction为 \lambda
s_new = \lambda * s_old + 1
```cpp
this->blobs_[2]->mutable_cpu_data()[0] *= moving_average_fraction_;
this->blobs_[2]->mutable_cpu_data()[0] += 1;
```

3）BN的正则化作用：在BN中每个batch计算得到的均值和标准差都是对于全局均值和标准差的近似估计，这为我们最优解的搜索引入了随机性，从而起到了正则化的效果。
4）BN的缺陷：当BN层的batchsize过小时，分类错误率会显著增大。当硬件受限不得不用较小的batchsize时，可以考虑用LayerNorm，InstanceNorm，GroupNorm。
GroupNorm的输出维度可能是2N，3N这种，在C通道上进行分组。

BN局限性：
BN并不适用于RNN等动态网络和batchsize较小的时候效果不好。


BN优点
1）更容易收敛，更快收敛，加速了训练过程
2）模型泛化性能更好，比没有bn 或者比用 dropout要好。


### batchnorm局限性，以及与LayerNorm应用场景的区别
https://zhuanlan.zhihu.com/p/152232203

BatchNorm，在HNHW维度归一化，保留了C维度。
InstanceNorm，在HW维度归一化，保留了NC维度。
LayerNorm，在CHW维度归一化，保留了N维度。

1）深度学习的正则化方法就是 “通过把一部分不重要的复杂信息损失掉，依次来降低拟合难度以及过拟合程度 ，从而加速了模型的收敛”。normalization的目的就是让分布稳定下来（降低各维度数据的方差）。
不同正则化方法区别只是操作的信息维度不同。
2）在CV中常使用BN，它在NHW维度进行了归一化，而channel维度的信息保存下来。因为在CV应用场景中， 数据在不同channel中的信息很重要，如果对其进行归一化会损失不同channel的差异信息。
3）NLP中不同batch样本的信息关联度不大，而且由于不同的句子长度不同，强行归一化会损失不同样本间的差异信息，所以就没再batch维度进行归一化。
4）LN，只考虑句子内部维度的归一化。可以认为NLP应用场景下一个样本内部维度间是有关联的，在CHW维度进行归一化能对不重要的信息损失掉，反而能降低方差。



### svm, random forest, gbdt原理部分
RF和gbdt都有通过随机特征或者样本构造多样性。


集成学习里主要有两个分支，bagging和boosting。
bagging用强分类器。
boosting用多个弱分类器来组成强分类器。要求基学习器分类误差必须低于0.5，要求学习基学习模型的差异性。


boosting主要是降低bias,对降低variance帮助不大，所以一般用欠拟合的弱分类器（同时计算代价也小，可以结合更多的分类器）。
bagging可以用来降variance, 用来结合强分类器很正常。
另外其实要满足weak learner的条件并没有那么容易



boosting为何要用弱分类器：
参考周老师机器学习里面的观点说一说。强学习器学习得到的模型往往趋同，也就是两个强学习器，对于同一批数据学习得到的模型很可能非常类似。
模型a可以分对的点模型b也可以分对，模型a错分的点模型b可能也是错的，这样就导致了集成学习没有意义了。而如果用弱学习器，可能模型a b分对的点都不多，但是他们可以分对的点不一样，这样集成学习做预测的时候可以综合两个弱学习器的优点提高分类准确率，甚至达到超越强学习器的效果。这也就是所谓的三个臭皮匠顶个诸葛亮，要达到这个效果就需要基学习器之间合而不同，而这也是弱学习器能够提供给我们的。

通俗举例：
在某一场考试中，ABCD四位同学一起作弊想考个高分。那么他们的最终答案就类似与bagging这种形式产生结果。如果ABCD四个同学都是学渣，那么这四个学渣很容易投票出来一个答案还是错的。只有四个同学水平中上，比如A同学可以判定答案a肯定错的，其他不能确定。B同学能判定b肯定是错的，以这种形式的话。那么这样做题的准确率才会高，所以此时会要求基学习器分类误差必须低于0.5。
所以在这种情况下，除了要求基学习器分类误差低于0.5之外，还要要求基学习器的差异性。
差异性在集成学习中还是非常重要的，stacking同理。


bagging方法一般用树作为基分类器，而不选用线性分类器，为何：
1）bagging对于弱分类器的要求是：它能在训练集上表现出低bias 高方差的能力(所谓的overfitting倾向)，未剪枝的树模型有这个特性。
线性模型在大训练集上很难做到overfitting，所以线性模型不适合bagging中单分类器的角色




random forest，如果单个分类器过拟合了，总效果是更加过拟合，还是过拟合减轻一些？



### gbdt原理
gbdt, gradient boosting decision tree,梯度提升决策树。使用的是boosting的思想。

1）boosting方法训练基分类器时采用串行方式，各个基分类器之间有依赖。基本思路时将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给与更高的权重。测试时，根据各层分类器的结果加权得到最终结果。
bagging与boosting串行训练方式不同，bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。


举例：
比如我今年30岁了，但计算机或者模型GBDT并不知道我今年多少岁，那GBDT咋办呢？
1）它会在第一个弱分类器（或第一棵树中）随便用一个年龄比如20岁来拟合，然后发现误差有10岁；
2）接下来在第二棵树中，用6岁去拟合剩下的损失，发现差距还有4岁；
3）接着在第三棵树中用3岁拟合剩下的差距，发现差距只有1岁了；
4）最后在第四课树中用1岁拟合剩下的残差，完美。
5）最终，四棵树的结论加起来，就是真实年龄30岁（实际工程中，gbdt是计算负梯度，用负梯度近似残差）。


gbdt优点
1）预测阶段的计算速度快，树与树之间可并行化计算(why)
2) 在分布稠密的数据集上，泛化能力和表达能力都较好，这使得gbdt在kaggle众多竞赛中经常名列榜首。
3) 采用决策树作为弱分类器，使得gbdt有较好的解释性和鲁棒性。
能够自动发现特征间的高阶关系

gbdt局限性
1）gbdt在高维度稀疏的数据集上，表现不如 SVM 或者 dnn
2）gbdt在处理文本分类特征问题上，相对于其他模型的优势不如它在处理数值特征时明显
3）训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。


### gbdt与random forest联系与区别
相同点
都由多棵树组成，最终的结果都是由多棵树一起决定

不同点
1）组成随机森林的树可以是分类树，也可以是回归树，而gbdt只由回归树组成
2）随机森林的结果是多数表决决定的，gbdt是多棵树累加之和。
3）随机森林对于异常值不敏感，gbdt对异常值敏感
4）随机森林减少模型的方差，gbdt减少模型的偏差
5）随机森林不需要进行特征归一化，gbdt需要特征归一化

6）随机森林的训练可以并行，gbdt训练时串行的




### 梯度提升与梯度下降的区别和联系
1）梯度下降和梯度提升，都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新。
2）梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参数的更新。
梯度提升中，模型是直接定义在 函数空间的，从而大大扩展了可以使用的模型的种类。

### 模型的好坏
好的模型需要具备两个基本要素：一是要有好的精度（即好的拟合程度），二是模型要尽可能的简单（复杂的模型容易出现过拟合，并且更加不稳定）

因此，我们构建的目标函数右边第一项是模型的误差项，第二项是正则化项（也就是模型复杂度的惩罚项）


### SVM、random forest、xgboost性能差异

adaboost-> xgboost ->lightgbm

gbdt发展到xgboost。再发展到lightgbm。
xgboost是gb的一种高效系统实现。xgboost里面的基学习器可以用tree，也可以用线性分类器。（gbtree，gblinear）

xgboost把算法和系统实现都做得非常极致。



https://zhuanlan.zhihu.com/p/34534004


xgboost：
1）xgboost还支持线性分类器，而传统gbdt是用cart作为基分类器，线性分类器效果是比cart性能要好的（不确定）。
从分类器的角度来看，xgboost相当于带L1 L2 正则化项的logistic回归（分类问题）或者线性回归（回归问题）

2)损失函数：
传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。


3）正则化
xgboost在代价函数里加入正则项，用于控制模型的复杂度。
正则项包含了叶子节点个数、每个叶子节点输出的score的L2的平方和。
从bias-variance tradeoff，正则项降低了模型的方差，使学习得到的模型更加简单，泛化能力更强，防止过拟合。
这是xgboost优于传统gbdt的一个特性。


3）shrinkage，相当于学习速度（xgboost中的eta）。
xgboost在进行完一次迭代后， 会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。(传统gbdt的实现也有学习速率)


实际应用中，一般把eta 设置得小一点，然后迭代
4）列抽样。column subsampling。xgboost借鉴了随机森林的做法，优点有两个，降低过拟合，减少计算量。


5）对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。

6）节点分裂方式：实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗。

节点分裂算法能自动利用特征的稀疏性。
7）并行计算。可并行的近似直方图算法。data事先排好序并以block的形式存储，利于并行计算。
cache-aware，out-of-core computation等。


xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。

这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。


忽略缺失值：在寻找splitpoint的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个工程技巧来减少了为稀疏离散特征寻找splitpoint的时间开销。


指定缺失值的分隔方向：可以为缺失值或者指定的值指定分支的默认方向，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，分到那个子节点带来的增益大，默认的方向就是哪个子节点，这能大大提升算法的效率。


并行化处理：在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。

那么如何得到优秀的组合树呢？

一种办法是贪心算法，遍历一个节点内的所有特征，按照公式计算出按照每一个特征分割的信息增益，找到信息增益最大的点进行树的分割。增加的新叶子惩罚项对应了树的剪枝，当gain小于某个阈值的时候，我们可以剪掉这个分割。但是这种办法不适用于数据量大的时候，因此，我们需要运用近似算法。

另一种方法：XGBoost在寻找splitpoint的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，按照特征值的密度分布，构造直方图计算特征值分布的面积，然后划分分布形成若干个bucket(桶)，每个bucket的面积相同，将bucket边界上的特征值作为split
point的候选，遍历所有的候选分裂点来找到最佳分裂点。

上图近似算法公式的解释：将特征k的特征值进行排序，计算特征值分布，rk（z）表示的是对于特征k而言，其特征值小于z的权重之和占总权重的比例，代表了这些特征值的重要程度，我们按照这个比例计算公式，将特征值分成若干个bucket，每个bucket的比例相同，选取这几类特征值的边界作为划分候选点，构成候选集；选择候选集的条件是要使得相邻的两个候选分裂节点差值小于某个阈值



xgboost与gbdt的区别
1) 基分类器
2) 损失函数
3) 正则约束
4) 列抽样
   
5) 缺失值处理
6) 节点分裂方式
7) 并行化：不是tree的并行，是特征维度的并行。



这个公式形式上跟ID3算法（采用entropy计算增益） 、CART算法（采用gini指数计算增益） 是一致的，都是用分裂后的某种值 减去 分裂前的某种值，从而得到增益。为了限制树的生长，我们可以加入阈值，当增益大于阈值时才让节点分裂，上式中的gamma即阈值，它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数lambda，是正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。


ID3: H(x) = -\sum p*logp  信息熵

特征A对于数据集D的经验条件熵H(D|A)
g = H(D) - H(D|A)


但是这种分割算法存在一定的缺陷：
假设每个记录有一个属性“ID”，若按照ID来进行分割的话，由于ID是唯一的，因此在这一个属性上，能够取得的特征值等于样本的数目，也就是说ID的特征值很多。那么无论以哪个ID为划分，叶子结点的值只会有一个，纯度很大，得到的信息增益会很大，但这样划分出来的决策树是没意义的。由此可见，ID3决策树偏向于取值较多的属性进行分割，存在一定的偏好。为减小这一影响，有学者提出C4.5的分类算法。


CART：以基尼系数为准则选择最优划分属性，可以应用于分类和回归
CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。

Gini(D)反映了数据集D的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。




GBDT是以决策树（CART）为基学习器的GB算法，是迭代树，而不是分类树。

https://zhuanlan.zhihu.com/p/29765582

GBDT的yuanli


cart为何可以做分类树和回归树使用




gbdt预测时每一棵树时能够并行的。GBDT指梯度提升决策树算法。


### gbdt, xgboost, lightgbm

https://zhuanlan.zhihu.com/p/56137208

xgboost 相比于gbdt优点
1）基分类器除了支持树分类器cart，还支持线性分类器。选择更广泛。
2）其可以选择正则项，带正则性的线性分类器，泛化能力更强。性能更好。
正则项降低了模型的方差。

3）列抽样，其和随机森林一样，这种计算量减小，降低过拟合。
4）近似直方图算法，可以并行计算。



lightdbm相比于xgboost优点
1）优于lightgbm 在直方图计算过程中，将连续的特征值分桶backets 装进离散的箱子bins，使得训练速度更快，内存占用更少。
它并行化能力强，处理大数据起来更游刃有余。

2）其通过leaf-wise分裂方法来产生树，其比level-wise分裂方法产生的树更复杂，能获得更高的准确率。
当然这种有时会导致过拟合，可以通过设置max-depth 参数来防止过拟合。




### 决策树做分类或回归
https://www.zhihu.com/people/ronalda-li

决策树就是一个类似于流程图的树形结构，树内部的每一个节点代表的是对一个特征的测试，属的分支代表特征的每一个测试结果，树的叶子节点代表一种分类结果。

决策树模型既可以做分类也可以做回归。

分类就是树模型的每个叶子节点代表一个类别。
回归就是根据特征向量来决定对应的输出值。回归树就是将特征空间划分成若干单元，每一个划分单元有一个特定的输出。对于测试数据，只要按照特征将其归到某个单元，便得到对应的输出值。





### bert介绍（bert201810）

Transformer介绍。6个encoder和6个decoder。encoder里有Multi-Head Attention，FeedForward-net. decoder里有Multi-Head Attention，FeedForward-net，还有encoder过来的attention。

Bert是双向transformer的encoder。模型的主要创新点在于pretrain上。其用了Masked LM和Next Sentence Prediction来分别捕捉词语和句子级别的表示。

其embedding层有三种嵌入，即 token  + segment + position embedding。segment用来区别句子。

预训练任务一    masked LM，这个任务就是为了得到语言模型。
在训练过程中作者随机mask 15%的token。而不是像cbow把每个词都预测一遍，最终的损失函数只计算被mask掉的那个token。

预训练任务二    next sentence prediction。这个任务的增加是为了让模型理解两个句子之间的联系。预测B是不是A的下一句。作者提到语料的选择很重要，要选用doc-level 而非 sentence-level的。


优点：
1）预训练和精调效果拔群。
1）其是transformer模型，相对RNN可以并行计算速度快、能捕捉长距离信息。双向的信息也非常充足。


### ftrl/adgrad，sgd，adam
CNN训练中经常用adam/sgd就够了。lr一般设置0.001 或者0.01，用step策略，分三步下降。
wide&deep中用的ftrl 和 adgrad。

ftrl和adgrad 都是学习率跟梯度累计成反比，更新越频繁学习率越低，都有学习率最终降低到0的问题。

ftrl收敛更慢，因为二阶正则希望离上一轮weight不要太远。

从网络结构上看，wide部分直接连接，梯度一步到位，收敛会比deep快，这个部分用ftrl，可以中和下，给deep部分时间学习更多信息。


ftrl with L1来训咯deep&wide的wide部分。
ftrl本身是一种稀疏性很好，精度也不错的随机梯度下降方法。
随机梯度下降，可以来一个样本就训练一次，进而实现模型的在线更新。

L1正则化，本身相比L2正则化，就能学习到更多稀疏性的特征。

wide&deep采用ftrl with L1 就是希望想让wide部分变得更加稀疏。
wide部分参数如果稀疏的好处

1）模型参数稀疏的话，就不用准备那些 0 权重的特征，模型保存占用空间会变小。


wide部分的特征非常重要：用户已安装app，曝光app。
他们是想发现当前曝光app和用户安装app的关联关系，以此来直接影响最终的得分。


deep部分的特征用稠密embedding，和年龄，app安装数量等稠密特征。

deep部分不希望用稀疏的特征向量。



### CF、矩阵分解、FM、LR
前深度学习时代，这些基础的东西还是要会。
性能 FM >  LR, FM > CF

背景：推荐相关的数据可能是稀疏的。



FM，因子分解机，是前深度学习时代在推荐领域应用得最多的模型，其较好的特征组合能力超越了协同过滤、逻辑回归的主流地位。

CF的地位是最早的，优点如下
1） 应用早，实现简单，可解释性强
2） 曾是许多大公司的主流推荐模型
3） 在某些场景下使用效果仍旧很好。

CF假设相似的人群拥有相似的兴趣爱好（物品）、相似的物品可以推荐给同一类人的思想展开推荐。

计算上，1）其将用户评价矩阵User-Item计算得到用户相似度矩阵User-User。相似度计算可以用余弦相似度及其他。2）然后计算用户偏好。用户对某item的评价分数 = 用户平均评价分数 + 其他所有用户与此用户相似度与其他用户评分偏差的加权和。

CF的性能上限是不高的。
协同过滤算法的原理和实现非常简洁，不需要设置损失函数， 梯度下降求最优解。要求的数据也非常简单（只要用户对商品的评分数据）。这也导致以下缺点。这些缺陷让CF失去主流地位。
1）表达能力不强。
这是只用了用户评分数据。相比那些利用了其他特征如用户年龄、姓名、职业、商品属性等这些信息训练出来的模型来讲，表达能力太弱了。
2）较难准确找到相似用户。
互联网应用场景下，用户的历史数据相对于总商品数而言极度稀疏，这是不利的。用户评分数据非常稀疏情形下，用户的相似性可能不高。

3）存在冷启动问题。
对于新用户、新商品， 没法利用用户评分数据，推荐质量不好。
4）需要维护一张巨大的用户相似度矩阵/商品相似度矩阵。（还好）


LR逻辑回归。
CF本质是利用用户和物品的相似度进行推荐。
逻辑回归，本质上是一个分类模型，其将推荐问题转化为一个分类问题来解决，或者说是一个点击率预估的问题。
梯度下降求解。模型参数量是N。


优点
1）其可以利用各种各样的特征。
综合用户、物品、时间、上下文等多维度信息进行模型训练。不局限于单一维度的评分信息。因此LR 比 CF 的表达能力强、泛化能力强。
2） 可解释性强。
可以直观看出特征的推荐权重，方便解释重要性。
3）模型简单，实现方便。
训练开销小，可并行化。


局限性
1）模型表达能力仍然不够。对特征的处理是简单粗暴的。
其实特征处理还可以做得更好的。
比如特征交叉。


POLY2模型：二阶交叉特征。参数量是N2。当缺乏足够的数据时，模型容易无法收敛。

因子分解机。
FM模型：有特征的一阶信息，也有二阶交叉信息。在二阶信息的处理上，FM用两个向量的内积代替单一的权重系数。这个操作把二阶权重参数量从N2 下降到 Nk。k << N. 这个处理的优势。 k为隐藏向量的维度。
1）参数量下降。从N2 下降到 NK
2）为每个特征学习了一个隐向量权重latent vector. 这种隐向量的计算方式可以更好解决数据的稀疏问题，从而提高模型的泛化能力。

为何可以解决稀疏性，提高泛化能力？
因为xi 和 xj 可能没有同时出现过，但 xi 和 xk 可能同时出现过。通过学习每个特征对应的embedding向量，再来求解 xi和 xj 同时出现的值。


FM的训练过程：
一阶部分可以用逻辑回归来训练。
二阶部分的训练(todo)


sigmoid函数有特性： 
1 - f(x) = f(-x)
f'(x) = f(x)(1 - f(x))



MF

### embedding设计



### word2vec采样

（1）CBOW 和 skip-gram


skip-gram: 通过中心词来预测其对应的上下文。有预测概率。取log最大化这个概率值。

skip-gram用softmax函数来计算该预测概率。




（2）分层softmax

Hierachical Softmax的基本思想就是首先将词典中的每个词按照词频大小构建出一棵Huffman树，保证词频较大的词处于相对比较浅的层，词频较低的词相应的处于Huffman树较深层的叶子节点，每一个词都处于这棵Huffman树上的某个叶子节点；
第二，将原本的一个|V|分类问题变成了[公式]次的二分类问题，做法简单说来就是，原先要计算[公式]的时候，因为使用的是普通的softmax，势必要求词典中的每一个词的概率大小，为了减少这一步的计算量，在Hierachical Softmax中，同样是计算当前词[公式]在其上下文中的概率大小，只需要把它变成在Huffman树中的路径预测问题就可以了，因为当前词[公式]在Huffman树中对应到一条路径，这条路径由这棵二叉树中从根节点开始，经过一系列中间的父节点，最终到达当前这个词的叶子节点而组成，那么在每一个父节点上，都对应的是一个二分类问题（本质上就是一个LR分类器），而Huffman树的构造过程保证了树的深度为[公式]，所以也就只需要做[公式]次二分类便可以求得[公式]的大小，这相比原来|V|次的计算量，已经大大减小了。

树的深度是logV。V是词典大小。这样就把 V分类问题变成了logV的分类问题。从而减小了计算量。


分层softmax是计算softmax问题的一种有效方法。该模型用二叉树来表示词汇表中的所有单词。
对于每个叶子节点，有一条唯一的路径可以从根节点到达该叶子节点；该路径被用来计算该叶子结点所代表的单词的概率。


（3）负采样
负采样是为了解决 数量太大的输出向量的更新问题。

负采样的思想，也是受了C&W模型中构造负样本方法启发，同时参考了Noise Contrastive Estimation (NCE)的思想，用CBOW的框架简单来讲就是，负采样每遍历到一个目标词，为了使得目标词的概率[公式]最大，根据softmax函数的概率公式，也就是让分子中的 [公式] 最大，而分母中其他非目标词的 [公式] 最小。

普通softmax的计算量太大就是因为它把词典中所有其他非目标词都当做负例了，而负采样的思想特别简单，就是每次按照一定概率随机采样一些词当做负例，从而就只需要计算这些负采样出来的负例了。


负采样相当于把V分类问题变成了 K分类问题。K是常数，很小。而V是词典大小。时间复杂度大大减小。



### wide&deep，deepfm，xdeepfm
wide&deep:
deep部分有非线性能力。wide部分利用浅层特征，基础特征和交叉特征。
deep是前馈神经网络。wide部分是线性的，是特征X的线性模型。其中X包括基础特征也包括交叉特征。

联合模型求解采用FTRL算法，L1正则。深度部分用adgrad优化器。

FTRL(Follow-the-regularized-Leader)




fm: fm要求二阶项系数是低秩的来对其加强限制，减少参数的自由度。（减小模型参数，增加模型的泛化能力）FM的求解可以用SGD求解。

FFM：增加Field的概念。相当于是对所有的特征分组了。这样交叉特征区分度会高些。


deepfm: DNN + FM。
1）相比wide&deep避免了手动设计交叉特征，虽然可能性能并不一定变强了。
2）相比wide&deep，选择了共享原始输入特征。
文中证明AUC 和 Logloss 都优于目前别的模型效果。效率上其与其他模型相当。


xdeepfm:




### DSSM
双塔模型。


## rcnn, fast-rcnn, faster-rcnn, faster-rcnn-fpn, yolov2

rcnn：每段都是分开的：ss提取候选框，CNN提取特征，SVM分类，BB回归。三个过程都需要分开训练，繁琐，总体性能不高。SS框提取是非常耗时的。2000个框，大概花费2s。
fast-rcnn：相当于两段：ss提取候选框， CNN提特征+softmax分类+ smoothL1回归，后面都是一起训练的。识别准确率得到提升。
faster-rcnn：end-to-end模型，把候选框的提取也让网络来完成。RPN + CNN + softmax + smoothL1Loss。准确率提升了，训练推理速度提升了。
faster-rcnn (resnet50) 成为目标检测的经典算法。


fast-rcnn相对于rcnn优点：
fast-rcnn是用传统图像处理得到很多候选框，候选框 + 图像特征提取 + 预测目标框和分类。因此
1）其分类和特征提取是连一起的，而rcnn是用svm分开的。


faster-rcnn相对于fast-rcnn：
1）faster-rcnn想着做一个end-to-end框架，把候选框提取+ 特征提取 + 预测框 + 预测类别都end-to-end。


faster-rcnn-fpn（res50）：
faster-rcnn对小目标学习不好。为了解决这个问题。
其相对于faster-rcnn增加了几个分支来训练预测。本质上相当于增加了很多的anchor来匹配目标框，让各种大小的目标框都能充分被训练。
结构：上采样-concat，预测分支。


yolov2：

yolo网络最终的全连接层的输出维度是 S*S*(B*5 + C)。YOLO论文中，作者训练采用的输入图像分辨率是448x448，S=7，B=2；采用VOC 20类标注物体作为训练数据，C=20。因此输出向量为7*7*(20 + 2*5)=1470维。



### Faster-RCNN、ResNet-FPN、Mask RCNN

FPN分为 bottom-up、top-down、横向连接三个部分。可以将各个层级的特征进行融合。
对小目标检测非常好。对于拼接，是高层stage 2x up, 然后与低一层的stage 1*1 conv换通道后 相加（不是concat估计用concat效果没啥提升还耗费显存，这里通道数我们用的256，在mmdetection中这里可以调整）。FPN的backbone常常是Res50（下采样32，有stage5），会引出P2/P3/P4/P5预测分支，预测class和bounding box。

Mask RCNN是在Faster RCNN上，backbone用的ResNet-FPN（也可以选择ResNet-50其他），head有边框界识别(分类和回归) + mask预测。mask预测分支就有上采样 和 通道数变少的过程。

MaskRCNN的另外一个改进是用ROIAlign。 RoiAlign相比 ROI pooling更能保留准确的空间位置。
MaskRCNN的损失函数多了Mask。mask分支的输出维度是 K*m*m，对于m*m的每个点，都输出k个二值mask(每个类别用sigmoid输出 )。计算loss的时候，并不是每个类别的sigmoid输出 都计算 二值交叉熵损失，而是只计算目标类的。

双线性插值。

Roi pooling有两次整数化过程：1）region proposal的xywh被整数化(可能有影响) 2）上述过程后有被平均分割成k*k个单元，整数化调整。
ROiAlign利用双线性插值，在上述两个过程中都保留浮点数进行计算。所以保留了精确的空间位置。


backbone基准的两个网络，resnet50 和mobilenet v1.

### 强化学习方差，方差比较大
1）强化学习的目标是要最大化累积奖励R_theta(tau)的期望值
eta 是一个随机变量，具有均值和方差。当多个随机变量相互独立时，方差有如下关系，
D(\sum X_i) = \sum(D(X_i)) 相当于是说多个随机变量组合到一起时，方差增大。

2）强化学习中，MC方法中是采用整个episode来估计R_theta(tau), 而在TD方法中只使用相邻的状态 s 来估计 R_theta(tau ), 随机变量 tau 是由多个随机变量s组成的，所以MC方法的方差比 TD方法的大。
虽然整个episode是无偏估计，但是方差大必然导致 估计期望时如果采样样本不足，估计值会不稳定。

3）强化学习中不管是做值优化还是策略优化，



### GAN难训练问题
GAN loss = log(D(x)) + (1-y)log(D(G(x)))
1）基本逻辑是G网络和D网络同事训练。D对于真实输入输出1，对G的输出而输出0. 刚开始G的性能不行，D很容易分类正确。如果G的性能逐渐变强，D得到有效训练。直到D对于G的输出作为输入时输出的概率是0.5时两个网络训练结束。
2） GAN存在G网络难以训练的问题


GAN难训练的原因：
1）G和D两个网络最佳状态是 纳什均衡，wgan/ wgan_gp/ wgan_div有些许进展。
2）理论上GAN可以收敛，但该优化过程是在函数空间完成的。
在实践中，我们的优化操作是在参数空间中进行的，理论上的保证在现实中并不总成立。对于有限个参数的判别器，GAN的平衡状态很可能是不存在的。

WGAN
W指的是Wasserstein距离。此距离数学性质比较好。

1) WGAN 能解决梯度消失的问题。
2) 


GAN训练经验
1）刚开始不要把D 判别器训练得太好，以避免后期梯度消失导致无法训练生成器G。
可以从f-divergence角度来看。
判别器的任务是辅助学习数据集的本质概率分布 和 生成器定义的隐式概率分布之间的某种距离， 生成器的任务是使得该距离达到最小。
当两个概率分布没有重合或者重合部分可忽略时，其f散度值为常数。当两者完全重合时，f散度值为0.

f散度为0时 无法为生成器提供可以减少损失函数的梯度信息，生成器无法或许优化方向。

可以认为数据集的本质概率分布和生成器定义的隐式概率分布均是高维数据空间中的低维流形，几乎不存在重叠部分( 重叠部分测度为0)，可以证明此时必然存在一个最优判别器D* 可以将两个分布完全分开，它在数据集的分布上置1 而在生成分布上置0，
即 P_data[D*(x)=1] = 1, P_g[D*(x)=0] = 1
而且在x 的邻域内其导数为0，即 grad_x D*(x) = 0

2）在有限采样个数的情况下（实际中样本数m 足够大是不可能成立的), 均匀分布的样本并不能等同于源分布，两者还是有一定的距离。
对于两个正态分布，很可能采样分布之间的距离并不等于两个分布之间的真实距离。

3）minmax问题
GAN的理想训练模式是这样的：固定生成器G，迭代 k次训练判别器D；然后固定判别器D，训练生成器G，两者依次交替使用梯度下降法进行更新。这里就有一个minmax 不等于maxmin的问题。

min_G max_D V(G,D) != max_D min_G V(G,D)

在maxmin角度来看，GAN训练过程会产生mode collapse问题，就是指生成器生成样本有大量的重复，多样性非常差（实践中常出现）

真实数据集的本质概率密度函数通常是多峰函数，也就是具有很多模式mode。



mode collapse问题的解决思路
此问题本质还是GAN训练优化问题。如果GAN可以收敛到最优的纳什均衡点，那mode collapse问题自然解决了。
生成数据的概率密度函数
训练数据集的概率密度函数

实际中很难达到全局最优解，一般收敛都是达到一个较好的局部最优解。
1）提高GAN学习能力，达到一个更好的局部最优解。
比较好的局部最优解有更多的模式，能一定程度上解决mode collapse问题。
如unrolled GAN，显式要求GAN学习更多的模式。
2）MAD-GAN思路，同时构造多个生成器，让每个生成器产生不同的模式，这样多个生成器结合起来可以保证样本具有多样性。
3）MAD-GAM-Sim 更强的一个版本，不仅考虑了每个生成器都分别负责生成不同的样本，而且更细致地考虑了样本的相似性问题。
其设计是 来自不同模式的样本应该看起来不同。





### ssim 两图相似性判别
在图像质量上作用较大，其比mse/L2得到的更好。
目前没有发现有效的更好的算法。





### spark的stage，容错机制

spark扩展了map-reduce过程，支持更多的计算模式。支持内存计算，比map-reduce更高效。

spark中的数据都抽象为RDD，有两大类的算子操作Transformation和Action。
Transformation算子的代码不会真正被执行，只有遇到action算子才会被执行。

Transformation算子有map, mapPartitions, flatMap, fliter, union, groupByKey, repartition, cache等
Action算子有：reduce，collect，count，foreach，saveAsTextFile

Transformation算子，本质上将一个父RDD转换成子RDD的过程。这个过程中，会有两种情况，父RDD中数据是否进入不同的子RDD，分别形成 wide dependency和narrow dependency。宽依赖一般成为shuffle依赖，如groupByKey等操作。


Job，stage，task
1）程序遇到一个action算子时，会提交一个job，执行前面一系列操作。 多个job之间串行执行。
2）一个job通常包含一个或多个stage。各stage之间按顺序执行。Job中stage的划分是依据shhuffle依赖进行的。
3）一个spark application提交后， 陆续被分解为job，stage，task。stage可以往下分解为task。task是 spark最细的执行单元了，task数量是 stage的并行度。


shuffle依赖是两个stage的分界点。shuffle操作一般是最消耗资源的部分。因为数据可能存放在HDFS不同的节点上，下一个stage 的执行首先要去拉取上一个stage的数据(shuffle read操作)，保存在自己的节点上，会增加网络通信和IO。

RDD在计算时，每个分区会启动一个task，rdd分区数=task数量。
每个task执行的结果时生成目标RDD的一个partition。




spark容错机制
容错原理：宽依赖，父RDD的一个分区对应一个子RDD的多个分区，如果出现部分计算结果丢失，便采用重新计算该步骤中的所有数据，会导致重复计算。窄依赖，父RDD分区只会被一个子RDD使用，如果出现本部分丢失，不需要计算所有数据，只计算出错部分的数据。

在调度层、RDD层、checkpoint层都有设计容错方案。
1）stage输出失败，上层调度器DAGScheduler重试
2）spark计算中，task内部任务失败，底层调度器重试。
3）RDD lineage窄依赖、宽依赖计算。
1）checkpoint缓存


spark性能优化
1）
2）
3）





要问的问题：

1. 不活跃用户的推荐（冷启动）
2. auc涨，但上线后指标不涨
3. 业务转化率是如何考虑的。转换率太低的问题。
4. 


CV难点：
小目标检测
重采样问题（对正样本和负样本如何采样）
灾难性遗忘（继承训练效果不佳）
蒸馏(大网络教小网络，一般来说小网络性能损失1%以上)


推荐难点问题：
采样问题（正样本和负样本采样）






