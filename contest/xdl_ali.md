
xdl库

### xdeeplearning典型特点

1）是面向高维稀疏数据的推荐/搜索/广告场景的深度学习的一整套方案。

针对大batch/ 低并发场景的性能优化，在此类场景下性能提升50-100%
存储及通信优化：参数无需人工干预自动全局分配，请求合并，彻底消除ps的计算/存储和通信热点
完整的流式训练特性：包括特征准入，特征淘汰，模型增量导出，特征counting统计等




1.2 与 1.1的改进

大batch/单样本海量特征场景性能优化
背景：


X-Deep Learning (XDL) 是针对工业级场景（比如搜索、推荐、广告）深度学习问题的解决方案，而不是与 TensorFlow、PyTorch、MXNet 并列的底层框架。实际上，XDL 采用桥接的方式支持使用 TensorFlow 和 MXNet 作为单节点的计算后端。如果应用场景是在小规模数据集上进行深度学习探索，比如 MNIST 数据集上的数字分类问题，TensorFlow 或者 PyTorch 应该是更好的选择。

那 XDL 针对的场景是什么呢？
百亿样本、千亿特征这种超大规模的工业级场景。

我这边简单罗列几个技术点：
1）对大规模样本进行离线训练时，如何在提高并发 worker 数量的同时获得近似线性的性能提升；

2）怎么处理大规模稀疏特征从 ID 到 Dense vector 的 Embedding Lookup 操作；

3）针对用户行为序列建模时，怎么设计离线样本的组织形式来兼顾存储效率和计算效率；

4）针对参数众多的复杂模型，除降低embeeding维度（16->8）和浮点数精度截断（FP32->FP16）之外，怎么在算法层面进行模型压缩。


在 XDL 开源前夕，机器之心采访了其团队的四位主要负责人：

靖世，研究员，阿里妈妈定向广告技术团队负责人兼阿里妈妈算法平台负责人
见独，资深技术专家，阿里妈妈工程平台技术负责人
怀人，资深算法专家，阿里妈妈算法平台深度学习方向负责人兼定向广告排序算法团队负责人
乐迪，资深技术专家，阿里妈妈大数据计算与机器学习平台的工程架构负责人



阿里巴巴深度模型的研发也是在尝试已有的框架，例如 Caffe、TensorFlow和 MXNet 等。但当时阿里巴巴发现已有框架在生产化方面有很多局限，首先第一个是大规模稀疏数据的处理能力，其次是如何实现结构化数据。

https://zhuanlan.zhihu.com/p/52964395


https://github.com/TPLink32/logistic-regression/blob/master/logistic_regression(github).md



### 基本模型





### lr问题

为何LR可以求解 ctr问题
把被点击的样本当成正例，把未点击的样本当成负例，那么样本的ctr实际上就是样本为正例的概率，LR可以输出样本为正例的概率，所以可以用来解决这类问题，另外LR相比于其他模型有求解简单、可解释强的优点，这也是工业界所看重的。



LR作为CTR预估的一个经典模型，他的理论和实际意义其它同学已经有了很好的回答，不累述了。

说一下我对CTR预估的一些浅见：无论是LR还是其它模型比如最近越来越受到青睐的FM, LR+GBDT, deep learning等方法，主要是解决的是rank order的问题，也就是模型给实际中点击概率高的样本的打分应该高于实际中点击概率低的样本。所以我们往往在评价这些模型的性能时，会使用AUC-ROC等指标。

然而在计算广告里，CTR预估往往不但要给出正确的rank order，还需要估计出实际的点击概率。但是不幸的事情是我们的观察数据是｛0，1｝的二元数据，永远无法知道他们背后的真实点击概率是多少。因此如何用二元的观察数据估计出实际的点击概率是个非常有挑战的事情。这种挑战在观察数据是有偏的情况下会变得更加棘手。比如RTB广告我们的观察数据往往受到bid landscape的影响，存在survival bias。

从这个意义上说，点击预估在过去一段时间大家主要的精力放在了rank order的精进上，却似乎有一些忽略了calibration的重要性。回到题主的问题，“为什么LR可以用来做CTR预估”，我想可能里面有一定程度的原因是经常有人提起“LR能够直接输出0-1的结果作为概率输出”，这里的前提是LR的log-linear假设是一个好的假设，也就是输入变量x=(x_1, ..., x_N)和条件概率P(y=1|x)之间的关系是满足log-linear假设的。然而如果实际中这个假设并不成立或者是个不怎么好的假设，LR输出的结果就不见得是一个好的点击概率估计。这个时候仍然需要非常精细的calibration。


### 搜索和排序

一面 技术面主要是围绕简历来问的，只要简历写的，都问了一遍。

1.word2vec原理
2.dssm讲解
3.用过bert吗
4.word2vec如何负采样


二面
1.parameter server原理，如何改进一致性，有master节点吗
2.lr模型推导。预测函数， 损失函数，优化算法
3.fm ffm原理
4.排序评价指标
5.map如何计算
6.word2vec使用huffman树优化后，和原模型是等价的吗，还是近似的
7.word2vec负采样后，损失函数是什么样的
8.合并多个有序数组

三面
1.ftrl为何能产生稀疏解
2.为何ftrl是基于l1 才能产生稀疏解
3.l1和l2 的区别
4.word2vec对正样本是如何采样的，为何
5.lr推导
6.线性回归的loss是啥，优化算法是啥
7.二叉树的序列化与反序列化
8.用过bert吗


### parameter server原理，如何改进一致性，有master节点吗
针对广告/推荐场景，一般采用Parameter Server的方式进行分布式训练，简称ps，ps可以同时做到对数据和模型的并行训练。

Parameter Server直译就是参数服务器，这里的参数，指的是模型权重，以及中间的过程变量。最近一段时间，我了解和使用了一些ps训练平台，在此做个总结，我会分成上下两篇对ps进行介绍。本文是上篇，主要是对论文[1]进行介绍，这是李沐在14年的文章。跟开发同学交流了解到，李沐开源的代码和文章的描述不完全一致，本文主要是基于文章进行介绍。



添加worker节点比较简单，步骤如下：

1）任务调度器给新的worker节点分配一部分训练数据；
2）新节点获取相应的训练数据，并开始进行训练；
3）任务调度器广播节点的变化，其他节点释放相应的训练数据



异步任务和灵活一致性
论文[1]中的异步训练主要是针对worker节点的，目前主流的ps系统，worker节点都是同步的，而server节点是异步的，下篇文章会进行介绍和对比。

下篇主要介绍目前主流的一些ps方案。这些方案与上篇的方案相比基本原理接近，但还是存在一些差异。目前的主流方案对系统做了简化，在一些细节的实现上，有更进一步的优化，针对具体的业务场景，增加了一些业务场景上的功能。这里主要介绍3个ps系统，阿里的XDL，360开源的TensorNet，和百度的PaddlePaddle。这3个系统有不同，但是也有诸多相似之处。

https://zhuanlan.zhihu.com/p/264828885










### word2vec负采样/层次softmax是近似还是等价
结论：都是近似

Word2vec 涉及到两种优化方式，一种是负采样，一种是层序Softmax

先谈一下负采样，以跳字模型为例。中心词生成背景词可以由两个相互独立事件的联合组成来近似（引自李沐大神的讲解）。

第一个事件是，中心词和背景词同时出现在窗口中。第二个事件是，中心词和K个噪声词不同时出现在窗口数据中，其中噪声词由噪声分布随机生成。

这里我们就可以知道上一个文章开头说到的，负采样是一种等价操作还是近似操作？我们在第二个事件中，使用了K个噪声词。但是实际上呢？应该远远大于K。

还是那个例子，句子为"我/永远/爱/中国/共产党"，中心词为'爱'，我们在选择噪声词的时候，选择了K个，但是实际上，在词汇表中，排除掉'我'，'永远'，'中国'，'共产党' 这四个词汇的其他词都可以算做我的噪声词，然而为了减少复杂度，我只选择了其中的K个，所以当然应该是近似了。



接下来，我们看层序Softmax。

层序Softmax 对应的就是在输出层使用一个霍夫曼树，代替了原本在输出层统一进行的softmax。

首先，我们需要了解霍夫曼树在这里是如何构建的。

简单讲，霍夫曼树是一个二叉树，以语料中出现过的词当做叶子节点，以各词在语料中出现的次数当做权值进行构造。其中叶子节点有N个，就是词典的大小，非叶子节点有N-1个（包括根节点）。

比如说我的所有文章中，“共产党”这个词出现了 100次，是最大的，那么根节点的左分支（或者右分支）就对应着”共产党“这个词，另一个分支做与根节点相同的操作，找到排除”共产党“这个词之外的所有词中最大的词，比如”中国“作为其中的左分支（或者右分支），以此类推，一个霍夫曼树就成功构建。

霍夫曼树中，我们需要注意的是，每个非叶子节点对应一个向量，每个叶子节点对应一个向量。两种向量都会随着模型的训练进行更新。

其中叶子节点的向量就是我们的词向量，而非叶子节点上的向量就是没有什么实际含义，它的作用就是帮助我们计算模型在霍夫曼树上不断的进行二分类时候的概率。

以上面那句话为例，我们现在中心词为‘爱’，然后，我要预测背景词‘中国’。首先我们要确定的是我的叶子节点是包含所有单词的，也就是包含了我这个简单句子的五个单词（不考虑前期数据清洗低频率词的情况）。

也就是说，在这个霍夫曼树上，有且仅有一条路径，让我从根节点出发，经过多次判断（也就是说走过了多个非叶子节点），最终走到了“中国”这个叶子节点，对应的概率就是每个节点概率的连乘。

然后这个时候，我们想一下霍夫曼树是不是一种近似？

当然是，我们每更新一个词向量，只是涉及到了可以到达叶子节点的这一条路径上节点。所以复杂度就是树的高度，也就是 O(log|V|)

1842831 8512

18510807878



### word2vec对正负样本采样

word2vec 训练时，更新窗口内的positive word 和 非窗口内negative words 对应的参数。如果不使用负采样， negative words 则为词典中，其他非窗口内所有的词，这个范围很大，训练非常耗时。而负采样方法 对 negative words进行随机抽样，降低 negative words 量级。

一个词被采样的概率，取决于这个词在语料中的词频 ，其满足一元分布模型(Unigram Model)。



### word2vec

https://zhuanlan.zhihu.com/p/29364112

cbow 比skip-gram 更快
为什么 cbow更快，很重要的一个原因，cbow是基于周围词来预测这个单词本身 。而skip-gram是基于本身词去预测周围词。 那么，cbow只要 把窗口内的其他词相加一次作为输入来预测 一个单词。不管窗口多大，只需要一次运算。而skip-gram直接受窗口影响，窗口越大，需要预测的周围词越多。在训练中，通过调整窗口大小明显感觉到训练速度受到很大影响。



2.2. 2 语言模型
skip-gram 和cbow,之前有对比，切词效果偏重各不相同。
从效果来看，感觉cbow对词频低的词更有利。这是因为 cbow是基于周围词来预测某个词，虽然这个词词频低，但是他是基于 周围词训练的基础上，通过算法来得到这个词的向量。通过周围词的影响，周围词训练的充分，这个词就会收益。


### GBDT+LR的分类效果一定比单独的GBDT、LR的效果好吗

一般情况下，调好参数的gbdt+lr是比单gbdt要好的。gbdt+lr中，gbdt是用来做特征组合和特征选择的，并将数据高纬化，使其变得线性可分。

在facebook那篇论文中，是因为gbdt不好做online learning，而lr很适合，所以采用离线train gbdt，用来做生成lr的特征，然后在线train lr这种组合方式。回到题主的问题，可以先看看你的gbdt有多少叶子节点，如果节点太少，数据有可能还是线性不可分的，这样加上lr后效果反而可能变差。



### xdl方案

深度兴趣网络 DIN
用户兴趣演化模型 DIEN
图像联合训练算法 CrossMedia
全空间多任务模型 ESMM
深度树匹配模型 TDM，Tree-based Deep Match
轻量级通用模型压缩算法 Rocket Training


DSSM


ESSM






